# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, SpeechBrain
# This file is distributed under the same license as the SpeechBrain
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SpeechBrain \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 13:45+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../API/speechbrain.nnet.attention.rst:5
msgid "speechbrain.nnet.attention module"
msgstr ""

#: of speechbrain.nnet.attention:1
msgid "Library implementing attention modules."
msgstr ""

#: of speechbrain.nnet.attention:6
msgid "Authors"
msgstr ""

#: of speechbrain.nnet.attention:4
msgid "Ju-Chieh Chou 2020"
msgstr ""

#: of speechbrain.nnet.attention:5
msgid "Jianyuan Zhong 2020"
msgstr ""

#: of speechbrain.nnet.attention:6
msgid "Loren Lugosch 2020"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:16
msgid "Summary"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:18
msgid "Classes:"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1
msgid ""
":obj:`ContentBasedAttention "
"<speechbrain.nnet.attention.ContentBasedAttention>`"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1 of
#: speechbrain.nnet.attention.ContentBasedAttention:1
msgid "This class implements content-based attention module for seq2seq learning."
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1
msgid ":obj:`KeyValueAttention <speechbrain.nnet.attention.KeyValueAttention>`"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1 of
#: speechbrain.nnet.attention.KeyValueAttention:1
msgid ""
"This class implements a single-headed key-value attention module for "
"seq2seq learning."
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1
msgid ""
":obj:`LocationAwareAttention "
"<speechbrain.nnet.attention.LocationAwareAttention>`"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1 of
#: speechbrain.nnet.attention.LocationAwareAttention:1
msgid ""
"This class implements location-aware attention module for seq2seq "
"learning."
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1
msgid ":obj:`MultiheadAttention <speechbrain.nnet.attention.MultiheadAttention>`"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1 of
#: speechbrain.nnet.attention.MultiheadAttention:1
msgid ""
"The class is a wrapper of MultiHead Attention for "
"torch.nn.MultiHeadAttention."
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1
msgid ""
":obj:`PositionalwiseFeedForward "
"<speechbrain.nnet.attention.PositionalwiseFeedForward>`"
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:35:<autosummary>:1 of
#: speechbrain.nnet.attention.PositionalwiseFeedForward:1
msgid ""
"The class implements the positional-wise feed forward module in "
"“Attention Is All You Need”."
msgstr ""

#: ../../API/speechbrain.nnet.attention.rst:37
msgid "Reference"
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:1
#: speechbrain.nnet.attention.KeyValueAttention:1
#: speechbrain.nnet.attention.LocationAwareAttention:1
#: speechbrain.nnet.attention.MultiheadAttention:1
#: speechbrain.nnet.attention.PositionalwiseFeedForward:1
msgid "Bases: :class:`torch.nn.modules.module.Module`"
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:4
msgid ""
"Refereance: NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND "
"TRANSLATE, Bahdanau et.al. https://arxiv.org/pdf/1409.0473.pdf"
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention
#: speechbrain.nnet.attention.ContentBasedAttention.forward
#: speechbrain.nnet.attention.KeyValueAttention
#: speechbrain.nnet.attention.KeyValueAttention.forward
#: speechbrain.nnet.attention.LocationAwareAttention
#: speechbrain.nnet.attention.LocationAwareAttention.forward
#: speechbrain.nnet.attention.MultiheadAttention
#: speechbrain.nnet.attention.MultiheadAttention.forward
#: speechbrain.nnet.attention.PositionalwiseFeedForward
msgid "Parameters"
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:7
#: speechbrain.nnet.attention.KeyValueAttention:10
#: speechbrain.nnet.attention.LocationAwareAttention:6
msgid "Size of the attention feature."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:9
#: speechbrain.nnet.attention.KeyValueAttention:12
#: speechbrain.nnet.attention.LocationAwareAttention:8
msgid "Size of the output context vector."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:11
#: speechbrain.nnet.attention.LocationAwareAttention:14
msgid "The factor controls the sharpening degree (default: 1.0)."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention:15
#: speechbrain.nnet.attention.KeyValueAttention:16
#: speechbrain.nnet.attention.LocationAwareAttention:18
#: speechbrain.nnet.attention.MultiheadAttention:21
#: speechbrain.nnet.attention.PositionalwiseFeedForward:17
msgid "Example"
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention.reset:1
#: speechbrain.nnet.attention.KeyValueAttention.reset:1
msgid "Reset the memory in the attention module."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention.forward:1
#: speechbrain.nnet.attention.KeyValueAttention.forward:1
#: speechbrain.nnet.attention.LocationAwareAttention.forward:1
msgid "Returns the output of the attention module."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention.forward:3
#: speechbrain.nnet.attention.KeyValueAttention.forward:3
#: speechbrain.nnet.attention.LocationAwareAttention.forward:3
msgid "The tensor to be attended."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention.forward:5
#: speechbrain.nnet.attention.KeyValueAttention.forward:5
#: speechbrain.nnet.attention.LocationAwareAttention.forward:5
msgid "The real length (without padding) of enc_states for each sentence."
msgstr ""

#: of speechbrain.nnet.attention.ContentBasedAttention.forward:7
#: speechbrain.nnet.attention.KeyValueAttention.forward:7
#: speechbrain.nnet.attention.LocationAwareAttention.forward:7
msgid "The query tensor."
msgstr ""

#: of speechbrain.nnet.attention.LocationAwareAttention:3
msgid ""
"Reference: Attention-Based Models for Speech Recognition, Chorowski "
"et.al. https://arxiv.org/pdf/1506.07503.pdf"
msgstr ""

#: of speechbrain.nnet.attention.LocationAwareAttention:10
msgid "Number of channel for location feature."
msgstr ""

#: of speechbrain.nnet.attention.LocationAwareAttention:12
msgid "Kernel size of convolutional layer for location feature."
msgstr ""

#: of speechbrain.nnet.attention.LocationAwareAttention.reset:1
msgid "Reset the memory in attention module."
msgstr ""

#: of speechbrain.nnet.attention.KeyValueAttention:4
msgid "Reference: \"Attention Is All You Need\" by Vaswani et al., sec. 3.2.1"
msgstr ""

#: of speechbrain.nnet.attention.KeyValueAttention:6
msgid ""
"Size of the encoder feature vectors from which keys and values are "
"computed."
msgstr ""

#: of speechbrain.nnet.attention.KeyValueAttention:8
msgid "Size of the decoder feature vectors from which queries are computed."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:3
msgid "Reference: https://pytorch.org/docs/stable/nn.html"
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:5
msgid "parallel attention heads."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:7
msgid "a Dropout layer on attn_output_weights (default: 0.0)."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:9
msgid "add bias as module parameter (default: True)."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:11
msgid "add bias to the key and value sequences at dim=0."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:13
msgid "add a new batch of zeros to the key and value sequences at dim=1."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:15
msgid "total number of features in key (default: None)."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention:17
msgid "total number of features in value (default: None)."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:1
msgid ""
"(N, L, E) where L is the target sequence length, N is the batch size, E "
"is the embedding dimension."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:4
#: speechbrain.nnet.attention.MultiheadAttention.forward:7
msgid ""
"(N, S, E) where S is the source sequence length, N is the batch size, E "
"is the embedding dimension."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:10
msgid ""
"(N, S) where N is the batch size, S is the source sequence length. If a "
"ByteTensor is provided, the non-zero positions will be ignored while the "
"position with the zero positions will be unchanged. If a BoolTensor is "
"provided, the positions with the value of True will be ignored while the "
"position with the value of False will be unchanged."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:17
msgid ""
"2D mask (L, S) where L is the target sequence length, S is the source "
"sequence length. 3D mask (N*num_heads, L, S) where N is the batch size, L"
" is the target sequence length, S is the source sequence length. "
"attn_mask ensure that position i is allowed to attend the unmasked "
"positions. If a ByteTensor is provided, the non-zero positions are not "
"allowed to attend while the zero positions will be unchanged. If a "
"BoolTensor is provided, positions with True is not allowed to attend "
"while False values will be unchanged. If a FloatTensor is provided, it "
"will be added to the attention weight."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:30
msgid ""
"(L, N, E) where L is the target sequence length, N is the batch size, E "
"is the embedding dimension."
msgstr ""

#: of speechbrain.nnet.attention.MultiheadAttention.forward:33
msgid ""
"(N, L, S) where N is the batch size, L is the target sequence length, S "
"is the source sequence length."
msgstr ""

#: of speechbrain.nnet.attention.PositionalwiseFeedForward:4
msgid ""
"Dimension of representation space of this positional-wise feed forward "
"module."
msgstr ""

#: of speechbrain.nnet.attention.PositionalwiseFeedForward:7
msgid "Expected shape of the input. Alternatively use ``input_size``."
msgstr ""

#: of speechbrain.nnet.attention.PositionalwiseFeedForward:9
msgid "Expected size of the input. Alternatively use ``input_shape``."
msgstr ""

#: of speechbrain.nnet.attention.PositionalwiseFeedForward:11
msgid "Fraction of outputs to drop."
msgstr ""

#: of speechbrain.nnet.attention.PositionalwiseFeedForward:13
msgid "activation functions to be applied (Recommendation: ReLU, GELU)."
msgstr ""

